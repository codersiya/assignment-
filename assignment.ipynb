{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2124,"sourceType":"datasetVersion","datasetId":1028}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport re\nimport string\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T07:59:48.904044Z","iopub.execute_input":"2025-04-09T07:59:48.904365Z","iopub.status.idle":"2025-04-09T07:59:48.909251Z","shell.execute_reply.started":"2025-04-09T07:59:48.904336Z","shell.execute_reply":"2025-04-09T07:59:48.908176Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# 1. Load and clean text\ndef load_and_clean_text(csv_path):\n    df = pd.read_csv(csv_path)\n    text = ' '.join(df['PlayerLine'].dropna().astype(str).tolist()).lower()\n    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub(r'\\d+', '', text)\n    text = re.sub(r'\\s+', ' ', text)\n    return text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T07:59:48.910581Z","iopub.execute_input":"2025-04-09T07:59:48.910888Z","iopub.status.idle":"2025-04-09T07:59:48.927043Z","shell.execute_reply.started":"2025-04-09T07:59:48.910856Z","shell.execute_reply":"2025-04-09T07:59:48.926463Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# 2. Tokenize and prepare sequences\ndef tokenize_text(text, sequence_length=5, max_sequences=20000):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts([text])\n    total_words = len(tokenizer.word_index) + 1\n\n    input_sequences = []\n    words = text.split()\n\n    for i in range(sequence_length, len(words)):\n        seq = words[i-sequence_length:i+1]\n        line = tokenizer.texts_to_sequences([' '.join(seq)])[0]\n        if len(line) == sequence_length + 1:\n            input_sequences.append(line)\n        if len(input_sequences) >= max_sequences:\n            break  \n\n    input_sequences = np.array(input_sequences)\n    X, y = input_sequences[:, :-1], input_sequences[:, -1]\n    y = to_categorical(y, num_classes=total_words)\n\n    return X, y, tokenizer, total_words\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T07:59:48.928780Z","iopub.execute_input":"2025-04-09T07:59:48.929005Z","iopub.status.idle":"2025-04-09T07:59:48.948046Z","shell.execute_reply.started":"2025-04-09T07:59:48.928986Z","shell.execute_reply":"2025-04-09T07:59:48.947489Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# 3. Build LSTM model\ndef build_model(total_words, seq_length):\n    model = Sequential()\n    model.add(Embedding(input_dim=total_words, output_dim=32, input_length=seq_length))\n    model.add(LSTM(32, return_sequences=True))\n    model.add(LSTM(16))\n    model.add(Dense(total_words, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T07:59:48.948846Z","iopub.execute_input":"2025-04-09T07:59:48.949135Z","iopub.status.idle":"2025-04-09T07:59:48.970016Z","shell.execute_reply.started":"2025-04-09T07:59:48.949111Z","shell.execute_reply":"2025-04-09T07:59:48.969335Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# 4. Generate text from a seed\ndef generate_text(seed_text, next_words, model, tokenizer, seq_length):\n    for _ in range(next_words):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        token_list = pad_sequences([token_list], maxlen=seq_length, padding='pre')\n        predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)[0]\n        output_word = ''\n        for word, index in tokenizer.word_index.items():\n            if index == predicted:\n                output_word = word\n                break\n        seed_text += ' ' + output_word\n    return seed_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T07:59:48.970681Z","iopub.execute_input":"2025-04-09T07:59:48.970873Z","iopub.status.idle":"2025-04-09T07:59:48.990266Z","shell.execute_reply.started":"2025-04-09T07:59:48.970857Z","shell.execute_reply":"2025-04-09T07:59:48.989467Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# 5. Main execution\nif __name__ == \"__main__\":\n    print(\" Loading and cleaning text...\")\n    dataset_path = \"/kaggle/input/shakespeare-plays/Shakespeare_data.csv\"  # Update path if needed\n    raw_text = load_and_clean_text(dataset_path)\n    print(\"Text cleaned. Length:\", len(raw_text))\n\n    print(\"\\n Tokenizing...\")\n    sequence_length = 5\n    X, y, tokenizer, total_words = tokenize_text(raw_text, sequence_length)\n    print(\" Tokenized. X shape:\", X.shape, \"| y shape:\", y.shape)\n    print(\" Vocabulary size:\", total_words)\n\n    print(\"\\n Building model...\")\n    model = build_model(total_words, sequence_length)\n\n    print(\"\\n Training model...\")\n    es = EarlyStopping(monitor='loss', patience=1, restore_best_weights=True)\n    model.fit(X, y, epochs=5, batch_size=64, callbacks=[es], verbose=1)\n\n    # 6. Generate and print text\n    seed = \"to be or not\"\n    generated = generate_text(seed, next_words=15, model=model, tokenizer=tokenizer, seq_length=sequence_length)\n    print(\"\\n Generated Output:\")\n    print(f'Seed: \"{seed}\"')\n    print(f'Generated: \"{generated}\"')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T07:59:48.991064Z","iopub.execute_input":"2025-04-09T07:59:48.991315Z","iopub.status.idle":"2025-04-09T08:00:29.546784Z","shell.execute_reply.started":"2025-04-09T07:59:48.991284Z","shell.execute_reply":"2025-04-09T08:00:29.545961Z"}},"outputs":[{"name":"stdout","text":" Loading and cleaning text...\nText cleaned. Length: 4164192\n\n Tokenizing...\n Tokenized. X shape: (20000, 5) | y shape: (20000, 27366)\n Vocabulary size: 27366\n\n Building model...\n\n Training model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5\n\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - accuracy: 0.0288 - loss: 9.0491\nEpoch 2/5\n\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.0321 - loss: 6.6044\nEpoch 3/5\n\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.0357 - loss: 6.4808\nEpoch 4/5\n\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.0358 - loss: 6.4328\nEpoch 5/5\n\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.0336 - loss: 6.4025\n\n Generated Output:\nSeed: \"to be or not\"\nGenerated: \"to be or not the the the the the the the the the the the the the the the\"\n","output_type":"stream"}],"execution_count":21}]}